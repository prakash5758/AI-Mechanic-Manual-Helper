# -*- coding: utf-8 -*-
"""RAG_QnA_Prelim_MLFlow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r8QzTtpQ_7SebHVyDnRrYo_J2c7k_tS3

# Installing the relevant libraries
"""

pip install langchain pypdfium2 tqdm seaborn spacy nltk mlflow  accelerate bitsandbytes xformers einops pyngrok evaluate rouge_score

pip install transformers==4.30

!pip install -qU pinecone-client==2.2.4

pip install -U sentence-transformers



"""# Loading the relevant libraries"""

#Importing the relevant libraries

# Document Loaders
from langchain.document_loaders.pdf import PyPDFium2Loader
from langchain.document_loaders import DirectoryLoader

# Document Splitter
from langchain.text_splitter import RecursiveCharacterTextSplitter,SpacyTextSplitter,NLTKTextSplitter

# tqdm for tracking time
from tqdm import tqdm
import pandas as pd
import numpy as np

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns

# Hashing
import hashlib

# Torch and transformers
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline,AutoModelForSeq2SeqLM

# Models
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceInstructEmbeddings,SentenceTransformerEmbeddings

# Vector Store
from langchain.vectorstores import Pinecone
import pinecone

# Prompts
from langchain import PromptTemplate, LLMChain

# Retrievers
from langchain.chains import RetrievalQA

# Importing time and textwrap
import textwrap
import time
from datetime import datetime

# Mounting and extracting pdfs
import zipfile
from google.colab import drive
import os

#nltk
import nltk
nltk.download('punkt')

import locale
locale.getpreferredencoding = lambda: "UTF-8"

#mlflow tracking
import mlflow
import nltk.translate.bleu_score as bleu
import re

#evaluation
import evaluate

#semantic similarity
from sentence_transformers import SentenceTransformer, util

#torch and gc
import gc

from pyngrok import ngrok

# Displaying the entire column in a dataframe
pd.set_option('display.max_colwidth', None)

"""# Loading the pdf documents"""

#Function for loading pdfs

def pdf_load_extract(pdf_loader,pdf_path,progress_display=True,multi_thread=False):

  drive.mount('/content/drive/')

  loader = DirectoryLoader(pdf_path,
                         glob="**/*.pdf",
                         use_multithreading=multi_thread,
                         show_progress=progress_display,
                         loader_cls=pdf_loader)
  docs = loader.load()

  return docs

#Loading pdf documents
path = '/content/drive/My Drive/Gen AI Case Study/PDF Documents/'
loader = PyPDFium2Loader

doc_start_time = datetime.now()
pdf_documents = pdf_load_extract(loader,path)
doc_end_time = datetime.now()

print('Time elapsed (hh:mm:ss.ms) {}'.format(doc_end_time - doc_start_time))

"""# Extracting Additonal Metadata (In addition to the page number)


*   Filename
*   Set
*   Category


"""

# Function for extracting metadata
def metadata_extraction(docs):

  metadata_list = []

  for i in range(len(docs)):
    #Adding the filename
    docs[i].metadata['filename'] = docs[i].metadata['source'].split("/")[-1]
    #Adding the Set
    docs[i].metadata['set'] = docs[i].metadata['source'].split("/")[-2]
    #Adding the category for set 1
    if docs[i].metadata['set'] == 'Set1':
      docs[i].metadata['category'] = docs[i].metadata['filename'].split("_")[2]
    else:
      docs[i].metadata['category'] = 'NA'
    #Adding the charcter count
    docs[i].metadata['charcount'] = len(docs[i].page_content)
    metadata_list.append(docs[i].metadata)

  return metadata_list

# Extracting metadata as a dataframe
docs = pdf_documents
metadata_df = pd.DataFrame(metadata_extraction(docs))
print(metadata_df.shape)

"""# EDA on the Pdf Documents

* Average number of pages
* Missing documents or documents that could not be parsed
* Page count distributions
* Character count distributions
* Token count distributions
"""

#avg pages across set 1 & set 2 files (87 pdfs)
total_pages_by_file = metadata_df.groupby(["set","filename"]).agg({"page":"count"}).reset_index()
# total_pages_by_file["page"].sum()/total_pages_by_file.shape[0]
total_pages_by_file.groupby(["set"]).agg({'page':"mean"})

# Any documents missing while reading?
set_a = set(os.listdir(path+ 'Set1/') + os.listdir(path+ 'Set2/'))
set_b = set(total_pages_by_file["filename"].tolist())
set_a - set_b

#Page counts across set 1 and set 2 files
plt.figure(figsize=(20,5))
plt.xticks(rotation=0)
ax = sns.barplot(total_pages_by_file,
            x=total_pages_by_file.index,
            y=total_pages_by_file["page"],
            hue=total_pages_by_file["set"])

#Distribution of files by #pages

def annotate_bars(ax=None, fmt='.2f', **kwargs):
    ax = plt.gca() if ax is None else ax
    for p in ax.patches:
         ax.annotate('{{:{:s}}}'.format(fmt).format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                     xytext=(0, 5),textcoords='offset points',
                     ha='center', va='center', **kwargs)


def dist_plot(annotate_bars,data,groupby_col,agg_col):

    g = sns.displot(data, x=agg_col, hue='set',kde=True)
    g.map(annotate_bars, fmt='.2g', fontsize=8, color='k')
    g.fig.set_size_inches(7,4)


    x1,x2 = data.groupby(groupby_col).agg({agg_col:"mean"}).values

    g.axes[0, 0].axvline(x1, color='blue', lw=2)
    g.axes[0, 0].axvline(x2, color='orange', lw=2)

    g.axes[0, 0].text(x1,20,x1,color='blue')
    g.axes[0, 0].text(x2,20,x2,color='orange')

dist_plot(annotate_bars,total_pages_by_file,"set","page")

#Average Characters and tokens by file df
char_by_file = metadata_df.groupby(["set","filename"]).agg({"charcount":"sum","page":"count"}).reset_index()

#Files that could not be parsed
print(char_by_file[char_by_file["charcount"]<1000])
files_not_parsed = char_by_file[char_by_file["charcount"]<1000]["filename"].tolist()

#Removing the files that could not be parsed and checking the average #pages, #characters, #tokens
char_by_file_reduced = char_by_file[~char_by_file["filename"].isin(files_not_parsed)]
char_by_file_reduced["tokencount"] = char_by_file_reduced["charcount"]/4
char_by_file_reduced.groupby(["set"]).agg({"charcount":["mean","min","max"],"page":["mean","min","max"],"tokencount":["mean","min","max"]})

#Distribution of files by characters
dist_plot(annotate_bars,char_by_file_reduced,"set","charcount")

#Distribution of files by token
dist_plot(annotate_bars,char_by_file_reduced,"set","tokencount")

"""# Chunking post loading the documents:

1.   Smaller chunks that can fit into your model's context window
2.   Chunks with as little noise as possible that are still semantically relevant
3.   Content is long form: pdfs
4.   Embedding models consideration
5.   Complexity of user queries: generally short and specific

**Different Methods:**

1.   Fixed Size Chunking: Decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them.
In general, we will want to keep some overlap between chunks to make sure that the semantic context doesnâ€™t get lost between chunks.

2.   Content Aware Chunking:

>      Sentence Splitting
>      a.  Naive splitting: By period (.)
>      b.  NLTK
>      c.  spaCy
>      Recursive Splitting: divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators

3.   Specialized Chunking:
Markdown and LaTeX

4.   Tiktoken -- works with Open AI models








"""

# Function for chunking the documents

def splitter(splitter_type,split_chunk_size,split_overlap,docs):

  if splitter_type =='recursive':

    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = split_chunk_size,
    chunk_overlap = split_overlap,
    length_function = len,
    separators=["\n\n", "\n", "\r\n","\t", "(?<=\. )"," ", ""]
    )
    texts = text_splitter.split_documents(docs)

  elif splitter_type =='spacy':

    text_splitter = SpacyTextSplitter(chunk_size=split_chunk_size)
    texts = text_splitter.split_documents(docs)

  elif splitter_type == 'nltk':

    text_splitter = NLTKTextSplitter(chunk_size=split_chunk_size)
    texts = text_splitter.split_documents(docs)

  return texts

#Splitting documents using the three splitters
#200,300,500,1000
#25%,50%,75%
ch_size = 300
ch_overlap = 150

rcts_start_time = datetime.now()
texts_rcts = splitter("recursive",ch_size,ch_overlap,docs)
rcts_end_time = datetime.now()

spacy_start_time = datetime.now()
texts_spacy = splitter("spacy",ch_size,ch_overlap,docs)
spacy_end_time = datetime.now()

nltk_start_time = datetime.now()
texts_nltk = splitter("nltk",ch_size,ch_overlap,docs)
nltk_end_time = datetime.now()


print('rcts time elapsed (hh:mm:ss.ms) {}'.format(rcts_end_time - rcts_start_time))
print('spacy time elapsed (hh:mm:ss.ms) {}'.format(spacy_end_time - spacy_start_time))
print('nltk time elapsed (hh:mm:ss.ms) {}'.format(nltk_end_time - nltk_start_time))

"""**Plotting the chunk distributions aross the 3 splitters**

*   Recursive Character Text Splitter
*   Spacy
*   NLTK


"""

#Comaring the chunk length distribution across the 3 splitters
doc_rcts_df = pd.DataFrame(pd.DataFrame([i.page_content for i in texts_rcts]))
doc_rcts_df["chunk_length"] = doc_rcts_df[0].apply(lambda x: len(x))

doc_spacy_df = pd.DataFrame(pd.DataFrame([i.page_content for i in texts_spacy]))
doc_spacy_df["chunk_length"] = doc_spacy_df[0].apply(lambda x: len(x))

doc_nltk_df = pd.DataFrame(pd.DataFrame([i.page_content for i in texts_nltk]))
doc_nltk_df["chunk_length"] = doc_nltk_df[0].apply(lambda x: len(x))

plt.figure(figsize=(20,5))
sns.histplot(doc_rcts_df, x= "chunk_length",binwidth=20, label = "rcts")
sns.histplot(doc_spacy_df, x= "chunk_length",binwidth=20, label = "spacy")
sns.histplot(doc_nltk_df, x= "chunk_length",binwidth=20, label = "nltk")

plt.legend()
# sns.histplot()

#RCTS
sns.histplot(doc_rcts_df, x= "chunk_length",kde=True,binwidth=3)

#Spacy
sns.histplot(doc_spacy_df, x= "chunk_length",kde=True,color='orange',binwidth=20)

#NLTK
sns.histplot(doc_nltk_df, x= "chunk_length",kde=True,color='green',binwidth=20)

#Spacy and nltk chunks > 300 (Number of chunks that exceed the 300 limit)
print("Number of spacy chunks > 300: " ,doc_spacy_df[doc_spacy_df['chunk_length']>300].shape[0])
print("Number of nltk chunks > 300: " ,doc_nltk_df[doc_nltk_df['chunk_length']>300].shape[0])
print("\n")
print("spacy chunk with the max length" ,max(doc_spacy_df[doc_spacy_df['chunk_length']>300]['chunk_length']))
print("nltk chunk with the max length" ,max(doc_nltk_df[doc_nltk_df['chunk_length']>300]['chunk_length']))

"""**Observations from chunking using the three splitters:**


*   RCTS: More consistent chunking. Does not breach the max chunk size. More uniform chunking.
*   Spacy & NLTK: Most chunks clustered around the max chunk size. However, it breaches the max chunk size for certain cases and there are outliers.


"""

#Metrics to track
#Average #charcters across pdfs
#Average #tokens across pdfs
#Average #pages
#Max tokens that can be passed to the llms(Based on Token size for the llms assigned)

"""# Loading the Model"""

# Function to get the model
def get_model(model):

    print('\nDownloading model: ', model, '\n\n')

    if model == 'llama-7b':
        model_repo = 'huggyllama/llama-7b'

        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit=True,
            torch_dtype=torch.float16,
        )

        max_len = 2048

    elif model == 't5-large':
        model_repo = 'google/flan-t5-large'

        tokenizer = AutoTokenizer.from_pretrained(model_repo, max_length=1024)

        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_repo,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        max_len = 1024


    else:
        print("Not implemented model (tokenizer and backbone)")

    return tokenizer, model, max_len

# %%time
# # model_start_time = datetime.now()
# tokenizer, model, max_len = get_model(model = "llama-7b")
# # model_end_time = datetime.now()

# # print('model time elapsed (hh:mm:ss.ms) {}'.format(model_end_time - model_start_time))

"""**Pipeline for loading the model**"""

#Hugging Face Pipeline

# max_new_tokens_t5 = 1024
# min_length_t5 = 1024
# temp = 0.6
# topp = 0.95
# rep_penalty = 1.15

def load_llm(model_name,model,tokenizer,max_len,temp,topp,rep_penalty):

  if model_name == "t5-large":

    pipe = pipeline(task = "text2text-generation",
                    model=model,
                    tokenizer=tokenizer)

    llm = HuggingFacePipeline(
        pipeline = pipe,
        model_kwargs={"do_sample": True,
                  "max_new_tokens":max_new_tokens_t5,
                  "min_length":min_length_t5,
                      },
        )

  elif model_name == "llama-7b":

    pipe = pipeline(
        task = "text-generation",
        model = model,
        tokenizer = tokenizer,
        pad_token_id = tokenizer.eos_token_id,
        max_length = max_len,
        temperature = temp,
        top_p = topp,
        repetition_penalty = rep_penalty,
        do_sample=True
        )

    llm = HuggingFacePipeline(pipeline = pipe)

  return llm

# %%time
# llm = load_llm(model_name = "llama-7b",model=model,tokenizer=tokenizer,max_len=max_len)

"""# Embeddings and Vector Store"""

# %%time
# ##Initializing the embeddings
# #Hugging Face Sentence Transformer
# embedding_model = "all-MiniLM-L6-v2"

# embedding_start_time = datetime.now()
# embeddings = SentenceTransformerEmbeddings(model_name = embedding_model)
# embedding_end_time = datetime.now()

# print('embedding initialization time elapsed (hh:mm:ss.ms) {}'.format(embedding_end_time - embedding_start_time))

"""**Pinecone vector store**"""

# #Pinecone
# #Initializing the pinecone client
# pinecone.init(
#             api_key= "59d977cf-c878-4da3-8e87-bc7726e2e2cd", # set api_key = 'yourapikey'
#             environment= 'gcp-starter'
# )
# id_name = 'index1'
# index_name = pinecone.Index(id_name)

# %%time
# #Uploading the embedding model and the data to the pinecone index
# #Gives indexing time
# #index_start_time = datetime.now()
# # vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name='index1')
# #index_end_time = datetime.now()

# #print('indexing time elapsed (hh:mm:ss.ms) {}'.format(index_end_time - index_start_time))

# #If index already created, load using from existing index function
# #Gives database loading time
# index_load_start_time = datetime.now()
# vectordb = Pinecone.from_existing_index(index_name="index1", embedding=embeddings)
# index_load_end_time = datetime.now()

# print('indexing time elapsed (hh:mm:ss.ms) {}'.format(index_load_end_time - index_load_start_time))

"""# Prompt Template"""

prompt_template = """
Don't try to make up an answer, if you don't know just say that you don't know.
Answer in the same language the question was asked.
Use only the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:"""


PROMPT = PromptTemplate(
    template = prompt_template,
    input_variables = ["context", "question"]
)

"""# Post Processing the Output & LLM Response Functions"""

#Function for post-processing the output
def wrap_text_preserve_newlines(text, width=700):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

def process_llm_response(llm_response):
    ans = wrap_text_preserve_newlines(llm_response['result'])

    sources_used = ' \n'.join(
        [
            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])
            for source in llm_response['source_documents']
        ]
    )

    ans = ans + '\n\nSources: \n' + sources_used
    return ans

# Function for getting the answer to the query
def llm_ans(qa_chain,query):
    start = time.time()
    llm_response = qa_chain(query)
    ans = process_llm_response(llm_response)
    end = time.time()

    time_elapsed = int(round(end - start, 0))
    time_elapsed_str = f'\n\nTime elapsed: {time_elapsed} s'
    return ans + time_elapsed_str,time_elapsed

"""# Answer Generation"""

def queries_and_labelled_data_processing(nonparsed_files,path_queries,path_labelled_set1,path_labelled_set2):

  #Load all the qa pairs (groundtruth)
  labelled_set1 = pd.read_excel(path_labelled_set1,sheet_name=None)
  labelled_set2 = pd.read_excel(path_labelled_set2,sheet_name="Misc.")

  #Load all queries
  queries = pd.read_excel(path_queries,sheet_name=None)

  #Processing labelled set 1 data
  files_not_parsed = [i.replace(".pdf","").replace("Black_Decker","Black&Decker") for i in nonparsed_files]

  queries_filtered_set1 = pd.DataFrame()
  for i in labelled_set1.keys():
    cols = ['Queries', 'Page Number',"Answers"]
    labelled_set1[i].loc[:,cols] = labelled_set1[i].loc[:,cols].ffill()
    labelled_set1[i] = labelled_set1[i][~labelled_set1[i]["Manual Name"].isin(files_not_parsed)]
    query_data = labelled_set1[i][["Ctegories","Queries","Answers","Page Number"]].drop_duplicates()
    queries_filtered_set1 = pd.concat([queries_filtered_set1,query_data],axis=0)
    queries_filtered_set1["Set"] = "Set1"

  queries_filtered_set1 = queries_filtered_set1.rename(columns={"Ctegories":"Category","Page Number":"Page no."})

  #Processing labelled set2 data
  labelled_set2 = labelled_set2[["Manual Name","Category","Queries","Answers","Page no."]]
  labelled_set2 = labelled_set2.ffill()
  queries_filtered_set2 = labelled_set2[["Category","Queries","Answers","Page no."]]
  queries_filtered_set2["Set"] = "Set2"

  #All set1 and set2 queries with answers
  queries_filtered = pd.concat([queries_filtered_set1,queries_filtered_set2],axis=0)
  queries_filtered = queries_filtered.dropna()
  queries_filtered["Category"] = queries_filtered["Category"].apply(lambda x: x.replace(" ",""))
  queries_filtered["Category"] = np.where(queries_filtered["Category"]=="AirCompressor","AirCompresssor",queries_filtered["Category"])
  queries_filtered["Queries"] = queries_filtered["Queries"].apply(lambda x : x.strip())

  #Processing queries
  queries["Queries_Set1"]["Set"] = "Set1"
  queries["Queries_Set2"]["Set"] = "Set2"

  queries["Queries_Set1"] = queries["Queries_Set1"].rename(columns=lambda x: x.strip())
  queries["Queries_Set2"] = queries["Queries_Set2"].rename(columns=lambda x: x.strip())
  queries_all = pd.concat([queries["Queries_Set1"],queries["Queries_Set2"]])
  queries_all = queries_all.reset_index(drop=True)


  #Sampling 100 queries
  queries_sampled = queries_all.sample(100,random_state=1)
  queries_sampled["Category"] = queries_sampled["Category"].apply(lambda x: x.replace(" ",""))
  queries_sampled["Category"] = np.where(queries_sampled["Category"]=="AirCompressor","AirCompresssor",queries_sampled["Category"])
  queries_sampled["Queries"] = queries_sampled["Queries"].apply(lambda x : x.strip())

  merged_queries = pd.merge(queries_sampled,queries_filtered,on=["Category","Queries","Set"], how='inner')
  merged_queries = merged_queries.drop_duplicates()

  merged_queries_wo_answers = merged_queries[['Category',"Queries","Set"]].drop_duplicates()

  return merged_queries,merged_queries_wo_answers

# set1_path = '/content/drive/MyDrive/Gen AI Case Study/Labelled Data/Labelled_Data_Set1.xlsx'
# set2_path = '/content/drive/MyDrive/Gen AI Case Study/Labelled Data/Labelled_Data_Set2.xlsx'
# query_path = '/content/drive/MyDrive/Gen AI Case Study/Queries/Queries.xlsx'

# merged_queries,merged_queries_wo_answers =  queries_and_labelled_data_processing(files_not_parsed,
#                                                                                  path_queries = query_path
#                                                                                  ,path_labelled_set1 = set1_path,
#                                                                                  path_labelled_set2 = set2_path)

def gen_answers(query_df,numk,ch_type,prompt):
  answer_list = []

  for i in tqdm(query_df.index):

    cat = query_df.loc[i,"Category"]
    query = query_df.loc[i,"Queries"]
    sett = query_df.loc[i,"Set"]

    settype = sett
    if sett == "Set1":
      cattype = cat
    else:
      cattype = "NA"


    #Retriever chain for question answering
    print(i,numk,settype,cattype)
    retriever = vectordb.as_retriever(search_kwargs = {"k": numk,
                                                        'filter': {'set':settype,
                                                                  'category':cattype}})

    qa_chain = RetrievalQA.from_chain_type(
        llm = llm,
        chain_type = ch_type,
        retriever = retriever,
        chain_type_kwargs = {"prompt": prompt},
        return_source_documents = True,
        verbose = False
    )


    ans = llm_ans(qa_chain,query)[0]
    answer_list.append([i,cat,query,sett,ans])
    torch.cuda.empty_cache()
    gc.collect()

  return answer_list

# answer_list =  gen_answers(query_df= merged_queries_wo_answers,
#                            numk = 3,
#                            ch_type = "stuff",
#                            prompt =  PROMPT)

# #Processing the answer list
# answer_df = pd.DataFrame(answer_list)
# answer_df_split_a = answer_df[4].str.split("\n\nSources:",expand=True).rename(columns={0:"Answer",1:"Source"})
# answer_df_split_b = answer_df_split_a["Source"].str.split("\n\nTime elapsed:",expand=True).rename(columns={0:"Source",1:"Time"})
# answer_df_split = pd.concat([answer_df_split_a["Answer"],answer_df_split_b],axis=1)
# answer_df = answer_df.drop([0],axis=1)
# answer_df.columns = ["Category","Query","Set","Answer_with_source"]
# answer_df_final = pd.concat([answer_df,answer_df_split],axis=1)
# answer_df_final["Answer"] = answer_df_final["Answer"].apply(lambda x : x.replace("\n\\end{code}",""))

"""# Answer Accuracy
* Rogue1: overlap of unigrams (each word) between the system and reference summaries.
* Rogue2: refers to the overlap of bigrams between the system and reference summaries.
* RogueL: Longest Common Subsequence (LCS).Longest common subsequence problem takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.
* Semantic Similarity:Semantic similarity between two pieces of text measures how their meanings are close. This measure usually is a score between 0 and 1. 0 means not close at all, and 1 means they almost have identical meaning.
* Bleu:a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations.
"""

def answer_accuracy(merged_query_eval_df):

  metrics = []

  for j in tqdm(range(merged_query_eval_df.shape[0])):


      #Example
      generated_summary = merged_query_eval_df.loc[j,"Answer"]
      reference_summary = merged_query_eval_df.loc[j,"Answers"].tolist()

      model_sim = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

      ss = []
      bleu_ = []
      rouge1_ = []
      rouge2_ = []
      rougeL_ = []
      for k in reference_summary:
          if len(generated_summary)==0:
             bleu_.append(0)
             rouge1_.append(0)
             rouge2_.append(0)
             rougeL_.append(0)
             ss.append(0)

          else:

            #BLEU
            bleu = evaluate.load("bleu")
            bleu_results = bleu.compute(predictions=[generated_summary], references=[k])
            bleu_.append(bleu_results["bleu"])

            #ROGUE
            rouge = evaluate.load('rouge')
            rogue_results = rouge.compute(predictions=[generated_summary], references=[k])
            rouge1_.append(rogue_results["rouge1"])
            rouge2_.append(rogue_results["rouge2"])
            rougeL_.append(rogue_results["rougeL"])

            #Semantic Similarity
            sentences = [generated_summary,k]
            sentence_embeddings = model_sim.encode(sentences)

            sentence_sim = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1])
            ss.append(sentence_sim.item())

      ss_avg = sum(ss) /len(ss)
      bleu_avg = sum(bleu_) /len(bleu_)
      rouge1_avg = sum(rouge1_) /len(rouge1_)
      rouge2_avg = sum(rouge2_) /len(rouge2_)
      rougeL_avg = sum(rougeL_) /len(rougeL_)

      metrics.append([bleu_avg,rouge1_avg,rouge2_avg,rougeL_avg,ss_avg])


  metrics_df = pd.DataFrame(metrics)
  metrics_df.columns = ["Bleu","Rouge1","Rouge2","RougeL","Semantic_Similarity"]

  return metrics_df

# merged_queries_eval_set = merged_queries.groupby(["Category","Set","Queries"])["Answers"].unique().reset_index()
# merged_queries_eval_set = pd.merge(merged_queries_eval_set,answer_df_final[["Category","Set","Query","Answer"]],left_on=["Category","Set","Queries"],right_on=["Category","Set","Query"])

# accuracy_df = answer_accuracy(merged_query_eval_df = merged_queries_eval_set)

"""###ML Flow Tracking"""

# Run mlflow
get_ipython().system_raw("mlflow ui --port 5000 &") # run tracking UI in the background

# Terminate open tunnels if exist
ngrok.kill()

token = "2Wzgz3sCG1L62G85LLsg9853cr6_6Gqg8xCMb4Veep9hfsGu2" # enter your api key (just simply sign in in the ngrok from google and get api)
# Setting the authtoken of ngrok
ngrok.set_auth_token(token)

# Open an HTTPs tunnel on port 5000 for http://localhost:5000
ngrok_tunnel = ngrok.connect(addr="5000", proto="http", bind_tls=True)
print("MLflow Tracking UI:", ngrok_tunnel.public_url)

ch_size = 1000 #200,300,500,1000
ch_overlap = 500 #25%,50%,75%
splitter_type = "recursive"

#Text Splitter
texts_rcts = splitter(splitter_type,ch_size,ch_overlap,docs)

# #Loading Model
# model_name = "llama-7b"

# tokenizer, model, max_len = get_model(model = model_name)

# #Model Pipeline
# max_new_tokens_t5 = 1024
# min_length_t5 = 1024
# temp = 0.6
# topp = 0.95
# rep_penalty = 1.15

# llm = load_llm(model_name = model_name,model=model,tokenizer=tokenizer,max_len=max_len)

##Initializing the embeddings
embedding_model = "all-MiniLM-L6-v2" #"all-MiniLM-L6-v2","BAAI/bge-large-en-v1.5"

embeddings = SentenceTransformerEmbeddings(model_name = embedding_model)

len(texts_rcts)

#Initializing the pinecone client
pinecone.init(
     api_key= "59d977cf-c878-4da3-8e87-bc7726e2e2cd",
     environment= 'gcp-starter'
 )
id_name = 'index1'

# #Uploading the embedding model and the data to the pinecone index
# vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name='index1')
index_name = id_name
DIMENSIONS = 384
# Create and configure index if doesn't already exist
if index_name not in pinecone.list_indexes():
    pinecone.create_index(
        name=index_name,
        metric="cosine",
        dimension=DIMENSIONS)
    vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name=index_name)

else:
    vectordb = Pinecone.from_existing_index(index_name, embeddings)


# index_name = pinecone.Index(id_name)
# vectordb = Pinecone.from_existing_index(index_name="index1", embedding=embeddings)

#Loading Model
model_name = "llama-7b"

tokenizer, model, max_len = get_model(model = model_name)

#Model Pipeline
# max_new_tokens_t5 = 1024
# min_length_t5 = 1024
temp = 0.7
topp = 0.1
rep_penalty = 1.15

llm = load_llm(model_name = model_name,model=model,tokenizer=tokenizer,max_len=max_len,temp=temp,topp=topp,rep_penalty=rep_penalty)

##Initializing the embeddings
embedding_model = "all-MiniLM-L6-v2" #"all-MiniLM-L6-v2"

embeddings = SentenceTransformerEmbeddings(model_name = embedding_model)

# #Initializing the pinecone client
# pinecone.init(
#             api_key= "59d977cf-c878-4da3-8e87-bc7726e2e2cd",
#             environment= 'gcp-starter'
# )
# id_name = 'index1'
# mlflow.log_params({
# "index": id_name
# })


# index_name = pinecone.Index(id_name)
# vectordb = Pinecone.from_existing_index(index_name="index1", embedding=embeddings)

#Initializing the pinecone client
pinecone.init(
    api_key= "59d977cf-c878-4da3-8e87-bc7726e2e2cd",
    environment= 'gcp-starter'
)
id_name = 'index1'


# #Uploading the embedding model and the data to the pinecone index
# vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name='index1')
index_name = id_name
DIMENSIONS = 384
# Create and configure index if doesn't already exist
if index_name not in pinecone.list_indexes():
    pinecone.create_index(
        name=index_name,
        metric="cosine",
        dimension=DIMENSIONS)
    vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name=index_name)

else:
    vectordb = Pinecone.from_existing_index(index_name, embeddings)

#Processing queries
set1_path = '/content/drive/MyDrive/Gen AI Case Study/Labelled Data/Labelled_Data_Set1.xlsx'
set2_path = '/content/drive/MyDrive/Gen AI Case Study/Labelled Data/Labelled_Data_Set2.xlsx'
query_path = '/content/drive/MyDrive/Gen AI Case Study/Queries/Queries.xlsx'

merged_queries,merged_queries_wo_answers =  queries_and_labelled_data_processing(files_not_parsed,
                                                                                path_queries = query_path
                                                                                ,path_labelled_set1 = set1_path,
                                                                                path_labelled_set2 = set2_path)

#Answer Generation
chaintype = "stuff"
num_k = 3

merged_queries_wo_answers_filtered = merged_queries_wo_answers[merged_queries_wo_answers["Queries"] == "How can I prepare the compressor to mount?"]

query_df= merged_queries_wo_answers_filtered
numk = 3
ch_type = chaintype
prompt =  PROMPT
for i in tqdm(query_df.index):

  cat = query_df.loc[i,"Category"]
  query = query_df.loc[i,"Queries"]
  sett = query_df.loc[i,"Set"]

  settype = sett
  if sett == "Set1":
    cattype = cat
  else:
    cattype = "NA"


  #Retriever chain for question answering
  print(i,numk,settype,cattype)
  retriever = vectordb.as_retriever(search_kwargs = {"k": numk,
                                                      'filter': {'set':settype,
                                                                'category':cattype}})

retriever.get_relevant_documents(query)

#Observations:
#Chunks of character size 300 too short (Misses out on a portion of the answer)
#

gen_answers(query_df= merged_queries_wo_answers_filtered,
                          numk = num_k,
                          ch_type = chaintype,
                          prompt =  PROMPT)

answer_list =  gen_answers(query_df= merged_queries_wo_answers,
                          numk = num_k,
                          ch_type = chaintype,
                          prompt =  PROMPT)

#Processing the answer list
answer_df = pd.DataFrame(answer_list)
answer_df_split_a = answer_df[4].str.split("\n\nSources:",expand=True).rename(columns={0:"Answer",1:"Source"})
answer_df_split_b = answer_df_split_a["Source"].str.split("\n\nTime elapsed:",expand=True).rename(columns={0:"Source",1:"Time"})
answer_df_split = pd.concat([answer_df_split_a["Answer"],answer_df_split_b],axis=1)
answer_df = answer_df.drop([0],axis=1)
answer_df.columns = ["Category","Query","Set","Answer_with_source"]
answer_df_final = pd.concat([answer_df,answer_df_split],axis=1)
answer_df_final["Answer"] = answer_df_final["Answer"].apply(lambda x : x.replace("\n\\end{code}",""))

#Evaluation
merged_queries_eval_set = merged_queries.groupby(["Category","Set","Queries"])["Answers"].unique().reset_index()
merged_queries_eval_set = pd.merge(merged_queries_eval_set,answer_df_final[["Category","Set","Query","Answer"]],left_on=["Category","Set","Queries"],right_on=["Category","Set","Query"])

accuracy_df = answer_accuracy(merged_query_eval_df = merged_queries_eval_set)
mean_accuracy = accuracy_df[["Bleu","Rouge1","Rouge2","RougeL","Semantic_Similarity"]].mean()

aa = "20230306"
mlflow.create_experiment(aa)

with mlflow.start_run(experiment_id=mlflow.get_experiment_by_name(aa).experiment_id):

        ch_size = 500 #300,500,1000
        ch_overlap = 250 #150,250,500
        splitter_type = "recursive"

        mlflow.log_params({
        "chunking size": ch_size,
        "chunking overlap": ch_overlap,
        "splitter":splitter_type
        })

        #Text Splitter
        texts_rcts = splitter(splitter_type,ch_size,ch_overlap,docs)

        #Loading Model
        model_name = "llama-7b"
        mlflow.log_params({
        "model_name": model_name
        })

        tokenizer, model, max_len = get_model(model = model_name)

        #Model Pipeline
        # max_new_tokens_t5 = 1024
        # min_length_t5 = 1024
        temp = 0.7
        topp = 0.1
        rep_penalty = 1.15

        mlflow.log_params({
        "temperature": temp,
        "topp": topp,
        "repetition_penalty":rep_penalty
        })

        llm = load_llm(model_name = model_name,model=model,tokenizer=tokenizer,max_len=max_len,temp=temp,topp=topp,rep_penalty=rep_penalty)

        ##Initializing the embeddings
        embedding_model = "all-MiniLM-L6-v2" #"all-MiniLM-L6-v2"
        mlflow.log_params({
        "embedding_model": embedding_model
        })
        embeddings = SentenceTransformerEmbeddings(model_name = embedding_model)

        # #Initializing the pinecone client
        # pinecone.init(
        #             api_key= "59d977cf-c878-4da3-8e87-bc7726e2e2cd",
        #             environment= 'gcp-starter'
        # )
        # id_name = 'index1'
        # mlflow.log_params({
        # "index": id_name
        # })


        # index_name = pinecone.Index(id_name)
        # vectordb = Pinecone.from_existing_index(index_name="index1", embedding=embeddings)

        #Initializing the pinecone client
        pinecone.init(
            api_key= "59d977cf-c878-4da3-8e87-bc7726e2e2cd",
            environment= 'gcp-starter'
        )
        id_name = 'index1'

        mlflow.log_params({
        "index": id_name
        })

        # #Uploading the embedding model and the data to the pinecone index
        # vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name='index1')
        index_name = id_name
        DIMENSIONS = 384
        # Create and configure index if doesn't already exist
        if index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=index_name,
                metric="cosine",
                dimension=DIMENSIONS)
            vectordb = Pinecone.from_documents(texts_rcts, embeddings, index_name=index_name)

        else:
            vectordb = Pinecone.from_existing_index(index_name, embeddings)

        #Processing queries
        set1_path = '/content/drive/MyDrive/Gen AI Case Study/Labelled Data/Labelled_Data_Set1.xlsx'
        set2_path = '/content/drive/MyDrive/Gen AI Case Study/Labelled Data/Labelled_Data_Set2.xlsx'
        query_path = '/content/drive/MyDrive/Gen AI Case Study/Queries/Queries.xlsx'

        merged_queries,merged_queries_wo_answers =  queries_and_labelled_data_processing(files_not_parsed,
                                                                                        path_queries = query_path
                                                                                        ,path_labelled_set1 = set1_path,
                                                                                        path_labelled_set2 = set2_path)

        #Answer Generation
        chaintype = "stuff"
        num_k = 3
        mlflow.log_params({
        "chain_type": chaintype,
        "num_k": num_k
        })
        answer_list =  gen_answers(query_df= merged_queries_wo_answers,
                                  numk = num_k,
                                  ch_type = chaintype,
                                  prompt =  PROMPT)

        #Processing the answer list
        answer_df = pd.DataFrame(answer_list)
        answer_df_split_a = answer_df[4].str.split("\n\nSources:",expand=True).rename(columns={0:"Answer",1:"Source"})
        answer_df_split_b = answer_df_split_a["Source"].str.split("\n\nTime elapsed:",expand=True).rename(columns={0:"Source",1:"Time"})
        answer_df_split = pd.concat([answer_df_split_a["Answer"],answer_df_split_b],axis=1)
        answer_df = answer_df.drop([0],axis=1)
        answer_df.columns = ["Category","Query","Set","Answer_with_source"]
        answer_df_final = pd.concat([answer_df,answer_df_split],axis=1)
        answer_df_final["Answer"] = answer_df_final["Answer"].apply(lambda x : x.replace("\n\\end{code}",""))

        #Evaluation
        merged_queries_eval_set = merged_queries.groupby(["Category","Set","Queries"])["Answers"].unique().reset_index()
        merged_queries_eval_set = pd.merge(merged_queries_eval_set,answer_df_final[["Category","Set","Query","Answer"]],left_on=["Category","Set","Queries"],right_on=["Category","Set","Query"])

        accuracy_df = answer_accuracy(merged_query_eval_df = merged_queries_eval_set)
        mean_accuracy = accuracy_df[["Bleu","Rouge1","Rouge2","RougeL","Semantic_Similarity"]].mean()

        mlflow.log_metric("Blue", mean_accuracy["Bleu"])
        mlflow.log_metric("Rouge1", mean_accuracy["Rouge1"])
        mlflow.log_metric("Rouge2", mean_accuracy["Rouge2"])
        mlflow.log_metric("RougeL", mean_accuracy["RougeL"])
        mlflow.log_metric("Semantic_Similarity", mean_accuracy["Semantic_Similarity"])

merged_queries_eval_set.to_excel('/content/drive/MyDrive/Gen AI Case Study/PDF Documents/llama_answers_20240306.xlsx')
accuracy_df.to_excel('/content/drive/MyDrive/Gen AI Case Study/PDF Documents/llama_answers_accuracy_20240306.xlsx')